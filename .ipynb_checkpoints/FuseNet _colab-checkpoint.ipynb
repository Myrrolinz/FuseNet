{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bd16b8a",
   "metadata": {},
   "source": [
    "# FuseNet: Self-Supervised Dual-Path Network for Medical Image Segmentation <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbea43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/mindflow-institue/FuseNet.git\n",
    "%cd ./FuseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c0cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utils import read_image, dice_metric, xor_metric, hm_metric, create_mask, cross_entropy\n",
    "from model_utils import Encoder, ProjectionHead, MixFFN_skip, CrossAttentionBlock\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a6f1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "parser = argparse.ArgumentParser(description='FuseNet: Self-Supervised Dual-Path Network for Medical Image Segmentation')\n",
    "parser.add_argument('--nChannel', metavar='N', default=64, type=int, \n",
    "                    help='number of channels')\n",
    "parser.add_argument('--maxIter', metavar='T', default=50, type=int, \n",
    "                    help='number of maximum iterations')\n",
    "parser.add_argument('--minLabels', metavar='minL', default=3, type=int, \n",
    "                    help='minimum number of labels')\n",
    "parser.add_argument('--lr', metavar='LR', default=0.005, type=float, \n",
    "                    help='learning rate')\n",
    "\n",
    "parser.add_argument('--input_path', metavar='INPUT', default='./input_images/', \n",
    "                    help='input image folder path')\n",
    "parser.add_argument('--save_output', metavar='SAVE', default=True, \n",
    "                    help='whether to save output ot not')\n",
    "parser.add_argument('--output_path', metavar='OUTPUT', default='./output/', \n",
    "                    help='output folder path')\n",
    "\n",
    "parser.add_argument('--loss_ce_coef', metavar='CE', default=2.5, type=float, \n",
    "                    help='Cross entropy loss weighting factor')\n",
    "parser.add_argument('--loss_clip_coef', metavar='AT', default=0.5, type=float, \n",
    "                    help='Clip loss weighting factor')\n",
    "parser.add_argument('--loss_b_coef', metavar='Spatial', default=0.5, type=float, \n",
    "                    help='Boundary loss weighting factor')\n",
    "\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefd34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.save_output:\n",
    "    SAVE_PATH = args.output_path\n",
    "    os.makedirs(SAVE_PATH, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af58b72",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4688dcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = args.input_path\n",
    "img_data = sorted(glob.glob(IMG_PATH + 'image/*'))\n",
    "lbl_data = sorted(glob.glob(IMG_PATH + 'GT/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d188db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_data), len(lbl_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29d88f9",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d3b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_dim (int): Dimension of the input data.\n",
    "        image_embed (int): Dimension of the image embeddings.\n",
    "        augmented_embed (int): Dimension of the augmented image embeddings.\n",
    "        input_size (tuple): Tuple representing the input size of the images (height, width).\n",
    "        temperature (float): Temperature parameter to scale CLIP matrix.\n",
    "        dropout (float): Dropout rate applied in the projection heads.\n",
    "        beta (int): Downsampling factor.\n",
    "        alpha (int): Scaling factor applied to the main path in the cross-attention block.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, image_embed, augmented_embed, input_size=(256, 256),\n",
    "                 temperature=5.0, dropout=0.1, beta=16, alpha=3):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        input_H, input_W = input_size\n",
    "        self.H = input_H\n",
    "        \n",
    "        self.beta = 16  # Downsampling factor\n",
    "        self.alpha = 3  # Main path scaling factor\n",
    "        self.img_enc = Encoder(input_dim, image_embed)\n",
    "        self.aug_enc = Encoder(input_dim, image_embed)\n",
    "        \n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embed, projection_dim=image_embed, dropout=dropout)\n",
    "        self.aug_projection = ProjectionHead(embedding_dim=augmented_embed, projection_dim=augmented_embed, dropout=dropout)\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        self.cross_attn = CrossAttentionBlock(in_channels=image_embed, key_channels=image_embed,\n",
    "                                              value_channels=image_embed, height=input_H, width=input_W)\n",
    "        \n",
    "        \n",
    "        self.patch_size = self.H//8 #32\n",
    "        self.dim = image_embed\n",
    "        patch_dim = self.dim * self.patch_size * self.patch_size\n",
    "        \n",
    "        self.to_patch_embedding_img = nn.Sequential(\n",
    "            Rearrange('b (h p1) (w p2) c -> b (h w) (p1 p2 c)', p1 = self.patch_size, p2 = self.patch_size),\n",
    "            nn.Linear(patch_dim, self.dim))\n",
    "        \n",
    "        self.to_patch_embedding_aug = nn.Sequential(\n",
    "            Rearrange('b (h p1) (w p2) c -> b (h w) (p1 p2 c)', p1 = self.patch_size, p2 = self.patch_size),\n",
    "            nn.Linear(patch_dim, self.dim))    \n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(image_embed)\n",
    "        self.bn2 = nn.BatchNorm2d(image_embed)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, augmented_x):\n",
    "\n",
    "        # extract feature representations of each modality\n",
    "        img_f = self.img_enc(x)\n",
    "        aug_f = self.img_enc(augmented_x) \n",
    "\n",
    "        img_f = rearrange(img_f, 'b c h w -> b (h w) c')\n",
    "        aug_f = rearrange(aug_f, 'b c h w -> b (h w) c')\n",
    "\n",
    "        # Getting Image and augmented image Embeddings (with same dimension)\n",
    "        img_e = self.image_projection(img_f)\n",
    "        aug_e = self.aug_projection(aug_f)\n",
    "                \n",
    "        # Calculating CLIP\n",
    "        img_e_r = self.bn1(rearrange(img_e, 'b (h w) c -> b c h w', h=self.H)).permute(0, 2, 3, 1)\n",
    "        aug_e_r = self.bn2(rearrange(aug_e, 'b (h w) c -> b c h w', h=self.H)).permute(0, 2, 3, 1)\n",
    "        \n",
    "        img_e_patch = self.to_patch_embedding_img(img_e_r) \n",
    "        aug_e_patch = self.to_patch_embedding_aug(aug_e_r) \n",
    "        \n",
    "        img_e_norm = img_e_patch / img_e_patch.norm(dim=-1, keepdim=True)        \n",
    "        aug_e_norm = aug_e_patch / aug_e_patch.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        clip_sim = (img_e_norm @ aug_e_norm.mT) / self.temperature\n",
    "        img_e_sim = img_e_norm @ img_e_norm.mT\n",
    "        aug_e_sim = aug_e_norm @ aug_e_norm.mT\n",
    "        clip_targets = F.softmax((img_e_sim + aug_e_sim) / 2 * self.temperature, dim=-1)\n",
    "        \n",
    "        # Cross attention\n",
    "        attn_1 = self.cross_attn(img_e*self.alpha, aug_e*0.8)\n",
    "        attn_2 = self.cross_attn(aug_e*0.8, img_e*self.alpha)\n",
    "        \n",
    "        attn = attn_1 + attn_2\n",
    "        \n",
    "        _, edge1 = torch.max(attn, 1)\n",
    "        attn_down = torchvision.transforms.functional.resize(attn, 256//self.beta, antialias=True)\n",
    "        attn_up = torchvision.transforms.functional.resize(attn_down, 256, antialias=True)\n",
    "        _, edge2 = torch.max(attn_up, 1)\n",
    "        edge = edge1 - edge2\n",
    "\n",
    "        return edge, attn, clip_sim, clip_targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45e692c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4808e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a4c435",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for img_num, img_file in enumerate(img_data):\n",
    "    \n",
    "    ##### Read image #####\n",
    "    image = read_image(img_file, img_size).to(device)\n",
    "\n",
    "    ##### Laod Model #####\n",
    "    model = Model(input_dim=3, image_embed=64, augmented_embed=64,\n",
    "                  input_size=(img_size, img_size), temperature=5.0, dropout=0.1,\n",
    "                  beta=16, alpha=3).to(device)\n",
    "    model.train()\n",
    "\n",
    "    ##### Setteings #####\n",
    "    zero_img = torch.zeros(image.shape[2], image.shape[3]).to(device)\n",
    "    \n",
    "    loss_ce = torch.nn.CrossEntropyLoss()\n",
    "    loss_s = torch.nn.L1Loss()\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\n",
    "    label_colours = np.random.randint(255, size=(128, 3))\n",
    "    \n",
    "    \n",
    "    jitter = T.ColorJitter(brightness=[1.4, 1.4], hue=[-0.06, -0.06])\n",
    "    aug_img = jitter(image)\n",
    "    aug_img = T.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5))(aug_img)\n",
    "    aug_img = aug_img.to(device)\n",
    "    \n",
    "    ##### Training #####\n",
    "    for batch_idx in range(args.maxIter):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        edge, output, clip_logits, clip_targets = model(image, aug_img)\n",
    "        \n",
    "        ### Output\n",
    "        output, clip_logits, clip_targets = output[0], clip_logits[0], clip_targets[0]        \n",
    "        output = output.permute(1, 2, 0).contiguous().view(-1, args.nChannel*2)\n",
    "                \n",
    "        _, target = torch.max(output, 1)\n",
    "        img_target = target.data.cpu().numpy()\n",
    "        img_target_rgb = np.array([label_colours[c % args.nChannel] for c in img_target])\n",
    "        img_target_rgb = img_target_rgb.reshape(image.shape[2], image.shape[3], image.shape[1]).astype(np.uint8)\n",
    "        \n",
    "        ### Cross-entropy loss function         \n",
    "        loss_ce_value = args.loss_ce_coef * loss_ce(output, target)\n",
    "        \n",
    "        ### Boundary Loss\n",
    "        loss_edge = args.loss_b_coef * loss_s(edge[0], zero_img)  \n",
    "        \n",
    "        ### CLIP loss \n",
    "        aug_loss = cross_entropy(clip_logits, clip_targets, 'mean')\n",
    "        img_loss = cross_entropy(clip_logits.T, clip_targets.T, 'mean')\n",
    "        loss_clip = args.loss_clip_coef * ((img_loss + aug_loss) / 2.0)\n",
    "        \n",
    "        ### Optimization        \n",
    "        loss = loss_ce_value + loss_clip + loss_edge\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        nLabels = len(np.unique(img_target))\n",
    "        print(batch_idx, '/', args.maxIter, '|', ' label num:', nLabels, ' | loss:', round(loss.item(), 4),\n",
    "                '| CE:', round(loss_ce_value.item(), 4), '| CLIP:', round(loss_clip.item(), 4),\n",
    "                '| B:', round(loss_edge.item(), 4))\n",
    "            \n",
    "        if nLabels <= args.minLabels and batch_idx>=5:\n",
    "            print (f\"Number of labels have reached {nLabels}\")\n",
    "            break\n",
    "        \n",
    "\n",
    "    ##### Evaluate #####\n",
    "    edge, output, _, _ = model(image, aug_img)\n",
    "    output = output[0].permute(1, 2, 0).contiguous().view(-1, args.nChannel*2)\n",
    "    _, target = torch.max(output, 1)\n",
    "    img_target = target.data.cpu().numpy()\n",
    "    img_eval_output = np.array([label_colours[c % args.nChannel] for c in img_target])\n",
    "    img_eval_output = img_eval_output.reshape(image.shape[2], image.shape[3], image.shape[1]).astype(np.uint8)\n",
    "    \n",
    "    \n",
    "    ##### Visualization #####\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(8, 8))\n",
    "    axes[0].imshow(img_eval_output)\n",
    "    axes[1].imshow(image[0].permute(1, 2, 0).cpu().detach().numpy()[..., ::-1])\n",
    "    axes[2].imshow(aug_img[0].permute(1, 2, 0).cpu().detach().numpy()[...,::-1])\n",
    "    axes[3].imshow(edge[0].cpu().detach().numpy())\n",
    "    axes[0].set_title('Prediction')\n",
    "    axes[1].set_title('Input Image')\n",
    "    axes[2].set_title('Augmented Image')\n",
    "    axes[3].set_title('Edge SR')    \n",
    "    axes[0].axis('off')\n",
    "    axes[1].axis('off')\n",
    "    axes[2].axis('off')\n",
    "    axes[3].axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    if args.save_output:\n",
    "        name = os.path.basename(img_file).split('.')[0]\n",
    "        cv2.imwrite(SAVE_PATH + '/FuseNet_mask_' + name + '.png', img_eval_output)\n",
    "        cv2.imwrite(SAVE_PATH + '/FuseNet_img_' + name + '.png', image[0].permute(1, 2, 0).cpu().detach().numpy()*255)\n",
    "        cv2.imwrite(SAVE_PATH + '/FuseNet_aug_' + name + '.png', aug_img[0].permute(1, 2, 0).cpu().detach().numpy()*255)\n",
    "        \n",
    "    print('-------------------------------', '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
